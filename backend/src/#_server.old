import express, { type Request, type Response } from 'express';
import multer from 'multer';
import cors from 'cors';
import pdf from 'pdf-parse';
import 'dotenv/config';

// LangChain & Google Gemini Imports
import { GoogleGenerativeAI } from '@google/generative-ai';
import { MemoryVectorStore } from 'langchain/vectorstores/memory';
import { GoogleGenerativeAIEmbeddings } from '@langchain/google-genai';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import type { Document } from 'langchain/document';

const app = express();
const port = 3001;

// --- Middlewares ---
app.use(cors());
app.use(express.json());

// Setup multer for in-memory file handling
const storage = multer.memoryStorage();
const upload = multer({ storage });

// --- Globals & Initializations ---
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY as string);
let vectorStore: MemoryVectorStore | null = null;

// --- PDF Processing Logic ---
const processPdf = async (pdfBuffer: Buffer): Promise<void> => {
  console.log('Starting PDF processing...');
  const data = await pdf(pdfBuffer);

  // 1. Split text into manageable chunks
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1500,
    chunkOverlap: 200,
  });
  const docs = await textSplitter.createDocuments([data.text]);

  // 2. Assign page numbers to each chunk (heuristic approach)
  const numPages = data.numpages;
  const processedDocs: Document[] = docs.map((doc, i) => {
    const page = Math.floor((i / docs.length) * numPages) + 1;
    return {
      ...doc,
      metadata: { ...doc.metadata, page },
    };
  });

  // 3. Create embeddings with Google's model and store in memory
  console.log('Creating embeddings and vector store...');
  const embeddings = new GoogleGenerativeAIEmbeddings({
    model: 'embedding-001',
  });
  vectorStore = await MemoryVectorStore.fromDocuments(
    processedDocs,
    embeddings
  );
  console.log('PDF processed and vectorized successfully.');
};

// --- API Endpoints ---

// Endpoint to upload and process a PDF
app.post(
  '/api/upload',
  upload.single('pdf'),
  async (req: Request, res: Response) => {
    if (!req.file) {
      return res.status(400).send({ message: 'No file uploaded.' });
    }
    try {
      await processPdf(req.file.buffer);
      res.status(200).json({ message: 'PDF processed successfully.' });
    } catch (error) {
      console.error('Error processing PDF:', error);
      res.status(500).send({ message: 'Error processing PDF.' });
    }
  }
);

// Endpoint to handle chat queries
app.post('/api/chat', async (req: Request, res: Response) => {
  const { query } = req.body;

  if (!query || !vectorStore) {
    return res
      .status(400)
      .json({ error: 'No document processed or no query provided.' });
  }

  try {
    console.log(`Searching for context for query: "${query}"`);
    // 1. Perform similarity search to find relevant context
    const searchResults = await vectorStore.similaritySearch(query, 5);
    const context = searchResults
      .map((result) => `[Page ${result.metadata.page}]: ${result.pageContent}`)
      .join('\n\n');

    // 2. Prepare the prompt for Gemini
    const prompt = `You are a helpful assistant. Answer the user's question based ONLY on the provided context from the PDF document.
You MUST cite the page number for each piece of information you use. Use the format [Page X] for citations.
If the context does not contain the answer, state that you cannot find the information in the document. Do not make up answers.

Context:
---
${context}
---

Question: ${query}
`;

    // 3. Call Gemini API with streaming
    const model = genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });
    const result = await model.generateContentStream(prompt);

    // Set headers for Server-Sent Events (SSE)
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.flushHeaders(); // Flush the headers to establish the connection

    for await (const chunk of result.stream) {
      const chunkText = chunk.text();
      res.write(`data: ${JSON.stringify(chunkText)}\n\n`);
    }
    res.end();
  } catch (error) {
    console.error('Chat API error:', error);
    res.status(500).json({ error: 'Failed to get response from AI.' });
  }
});

// --- Start Server ---
app.listen(port, () => {
  console.log(`Backend server running at http://localhost:${port}`);
});
